{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DOPADON9340/lunar-landing-AI-/blob/main/Lunar_Landing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZcI9ZXHl3a"
      },
      "source": [
        "# Deep Q-Learning for Lunar Landing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yPRjteXgPb"
      },
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slEm5teGWjWU"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb5dd94-20f5-424a-ca9d-d8e3e36fb6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.2)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2024.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=d9bafd9278106338dadacf72e25efd53afa4c6a3dfce6f63cf7a2b5ec0ac5e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (4,224 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install torch\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqiMN3UW9T9"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZaKXP_aMl9O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlDKXvkXzGI"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtG6Zc83YYy3"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "      super(Network, self).__init__()\n",
        "      self.seed = torch.manual_seed(seed)\n",
        "      self.fc1 = nn.Linear(state_size, 64)\n",
        "      self.fc2 = nn.Linear(64, 64)\n",
        "      self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "      x = self.fc1(state)\n",
        "      x = F.relu(x)\n",
        "      x = self.fc2(x)\n",
        "      x = F.relu(x)\n",
        "      return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "BD9g8MUN_u0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVrBnFWZKb1"
      },
      "source": [
        "## Part 2 - Training the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T364fz9qZb2j"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ],
      "metadata": {
        "id": "_CXHULaNRNtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_dZmOIvZgj-"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 100\n",
        "discount_factor = 0.99\n",
        "replay_buffer_Size = int(1e5)\n",
        "interpolation_parameter = 1e-3"
      ],
      "metadata": {
        "id": "RGASM3HnSgUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hD_Vs-bYnip"
      },
      "source": [
        "### Implementing Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k = batch_size)\n",
        "    states = torch.from_numpy(np.vstack( [e[0]for e in experiences if e is not None ])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack( [e[1]for e in experiences if e is not None ])).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack( [e[2]for e in experiences if e is not None ])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack( [e[3]for e in experiences if e is not None ])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack( [e[4]for e in experiences if e is not None ]).astype(np.uint8)).float().to(self.device)\n",
        "    return states, next_states, actions, rewards, dones"
      ],
      "metadata": {
        "id": "Ht4Z55fRTYrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmEkbFbUY6Jt"
      },
      "source": [
        "### Implementing the DQN class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(state_size, action_size)\n",
        "    self.target_qnetwork = Network(state_size , action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr= learning_rate)\n",
        "    self.memory = ReplayMemory(replay_buffer_Size )\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = ( self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      if len(self.memory.memory) > minibatch_size:\n",
        "        experiences = self.memory.sample(100)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences , discount_factor):\n",
        "    states, next_states , actions , rewards , dones = experiences\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + (discount_factor * next_q_targets *( 1 - dones))\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters() , local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n"
      ],
      "metadata": {
        "id": "m4eCSe-dq2E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tZElccZmf6"
      },
      "source": [
        "### Initializing the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size, number_actions )"
      ],
      "metadata": {
        "id": "U5hDABiyBIvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8v0PtUfaVQp"
      },
      "source": [
        "### Training the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value  = 1.0\n",
        "epsilon_ending_value  = 0.01\n",
        "epsilon_decay_value  = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 200.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "id": "fECdPQnmB9oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8CNwdOTcCoP"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v2')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}